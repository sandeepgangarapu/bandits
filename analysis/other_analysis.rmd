---
title: "MAB Simulations - More analysis"
author: "Sandeep Gangarapu"
output: html_document
---


```{r echo=FALSE, warning=FALSE}
suppressPackageStartupMessages(
  {
library(ggplot2)
library(dplyr)
library(tidyr)
library(directlabels)
library(gganimate)
library(cumstats)
  })
```

# Simulation set up

We have 10 arms which have a normal outcome distribution with different means and variances. The values of these true means and variances are given below.

Every time we pull an arm (make an allocation), we sample from the normal distribution of given by the mean and variance of that arm.

We see from the below true values that ARM-8 should the winning arm as it has the highest mean of 4.9.



```{r echo=TRUE}

true_means = c(3.36139279, 2.440392, 4.12747587, 0.25, 4.04024982, 2.8280871, 1.48811249, 0.2334786, 4.953137, 3)

true_vars = c(3.84896514, 3.7338355, 1.88719468, 2.47073726, 4.64474196, 1.97727022, 4.86978148, 2.62207358, 1, 4.06654206)

grp = c(0:9)
true_df <- data.frame(grp, true_means, true_vars)
true_df <- true_df %>% mutate(grp=factor(grp))

```

Time Horizon is the number of allocations we have available synonymous to number of experimental units in an experiment. 
In this simulation, we set time horizon to 2000 units.

We run this simulation for 20 different times using various seeds.



```{r echo=FALSE, cache=TRUE, warning=FALSE}
setwd("G:\\My Drive\\Research\\Bandits\\code\\bandits\\analysis\\output")

xi_simulation <- FALSE
horizon_analysis <- FALSE


data <- read.csv("sim_1_2000.csv")


seed_algs <- c("ab", "ucb", "thomp", "eps_greedy")
inf_algs <- c("ucb_inf_eps", "thomp_inf_eps")
est_algs <- c("thomp_ipw", "thomp_aipw", "thomp_inf_eps_ipw", "thomp_inf_eps_aipw", "thomp_eval_aipw", "thomp_inf_eps_eval_aipw")

group_outcome <- data %>% filter(alg %in% c(seed_algs, inf_algs)) %>%
  select(alg, group, ite, outcome) %>%
  group_by(ite,alg) %>% mutate(x=row_number()) %>%
  ungroup()

regret_mse <- data %>% filter(alg %in% c(seed_algs, inf_algs)) %>%
  select(alg, regret, ite, mean_mse, var_mse) %>%
  group_by(ite,alg) %>% mutate(x=row_number()) %>%
  ungroup()

prop_mse <- data %>% filter(alg %in% est_algs) %>% select(alg, ite, group, mse, mean_est) %>% 
  group_by(ite,alg) %>% mutate(x=row_number()) %>% ungroup()

xlimit <- max(group_outcome$x) + 500
  
if (xi_simulation) {
  xi_group <- read.csv("group_eps_sim.csv")
  xi_regret <- read.csv("regret_mse_eps_sim.csv")
}

if (horizon_analysis) {
  horizon <- read.csv("sim_1_20000.csv")
  regret_horizon <- horizon %>% select(alg, regret, ite, mean_mse, var_mse) %>%
  group_by(alg) %>% mutate(x=row_number())%>%
  ungroup()

  horizon_xlimit <- max(horizon$x) + 500

}


# Setting new WD so that results are saved in different folder
setwd("G:\\My Drive\\Research\\Bandits\\code\\bandits\\analysis\\results")


```




### Average number of optimal/sub-optimal allocations in each algorithm


```{r echo=FALSE, warning=FALSE}

df_alloc <- group_outcome %>% filter(alg == main_algs) %>% 
  mutate(group = ifelse(group==8, 'Optimal', 'SubOptimal') )%>% group_by(alg, group, ite) %>% count() %>% group_by(alg, group) %>% summarise(mean_alloc = mean(n))

ggplot(df_alloc, aes(x=alg, y=mean_alloc, fill=group)) + geom_bar(stat="identity", position="dodge") +
  theme_bw()  +
  labs(title = "Average number of Optimal/SubOptimal allocations")

```

UCB is the most optimal as it is efficient. AB does random allocation, so, it is the least optimal.


#### Average number of optimal/sub-optimal allocations in each algorithm


```{r echo=FALSE, warning=FALSE}

df_alloc <- group_outcome %>% filter(alg == adv_algs) %>% 
  mutate(group = ifelse(group==8, 'Optimal', 'SubOptimal') )%>% group_by(alg, group, ite) %>% count() %>% group_by(alg, group) %>% summarise(mean_alloc = mean(n))

ggplot(df_alloc, aes(x=alg, y=mean_alloc, fill=group)) + geom_bar(stat="identity", position="dodge") +
  theme_bw()

```



UCB_INF_EPS is slightly worse compared to UCB but is not as bad as Thompson Sampling based algorithms


The relative performance of these algorithms can be formalized in regret analysis.


## Incresing the horizon

```{r echo=FALSE, warning=FALSE, eval=horizon_analysis}

regret_mse1 <- data %>% select(alg, regret, ite, mean_mse, var_mse) %>%
  group_by(ite,alg) %>% mutate(x=row_number()) %>%
  ungroup()

df_regret1 <- regret_mse1 %>% filter(ite==0)


ggplot(df_regret1, aes(x=x, y=regret)) + geom_line(aes(color=alg)) +
  labs(title = "Regret") + xlim(0,horizon_xlimit) +
  geom_dl(aes(label=alg), method=list('last.points', cex=0.8)) +
  theme_bw() 


```

UCB_INF_EPS levels off while EPS_GREEDY continues to increse at linear rate.


# $\xi$ simulation for ucb_inf_eps

$\eta = \frac{\sum{\frac{\sigma^2}{n}}}{\xi K}$
$\epsilon_n = \frac{\eta}{1+\eta}$



```{r echo=FALSE, eval=xi_simulation, warning=FALSE}


xi_group_sub <- xi_group %>% mutate(chi = round(chi,2)) %>%
  group_by(chi) %>% mutate(x=row_number()) %>% ungroup()


ggplot(xi_group_sub) +
  geom_point(aes(x=x, y=factor(group)), shape=1, alpha=0.6) +
  facet_grid(chi ~.) + theme_bw() +
  labs(x='Time Period', y = 'Group') +
  theme(axis.text.y = element_text(size = 6)) + 
  theme(axis.text.y = element_text(size = rel(1))) 

xi_regret_sub <- xi_regret %>% 
  mutate(chi = factor(round(chi,2))) %>% group_by(chi) %>% 
  mutate(x=row_number())

ggplot(xi_regret_sub, aes(x=x, y=regret)) + 
  geom_line( aes(color=chi)) + theme_bw() + geom_dl(aes(label=chi), method=list('last.points', cex=0.5)) +labs(title = 'Regret vs Xi')

ggplot(xi_regret_sub, aes(x=x, y=mean_mse)) + 
  geom_line(aes(color=chi)) + geom_dl(aes(label=chi), method=list('last.points', cex=0.5)) +
  theme_bw() + ylim(0,0.25) +labs(title = 'MSE Zoomed')

```

$\xi$ serves as a knob for practitioner to control the amount of exploration in INF_EPS. $\xi$ directly effects $\epsilon$ which in turn effects the amount of exploration. Higher the value of $\xi$, lower the amount of exploration.


