---
title: "MAB Simulations"
author: "Sandeep Gangarapu"
output: html_document
---


```{r echo=FALSE, warning=FALSE}
suppressPackageStartupMessages(
  {
library(ggplot2)
library(dplyr)
library(tidyr)
library(directlabels)
  })
```

# Simulation set up

We have 10 arms which have a normal outcome distribution with different means and variances. The values of these true means and variances are given below.

Every time we pull an arm (make an allocation), we sample from the normal distribution of given by the mean and variance of that arm.

We see from the below true values that ARM-8 should the winning arm as it has the highest mean of 4.9.



```{r echo=TRUE}

true_means = c(3.36139279, 2.440392, 4.12747587, 0.25, 4.04024982, 2.8280871, 1.48811249, 0.2334786, 4.953137, 3)

true_vars = c(3.84896514, 3.7338355, 1.88719468, 2.47073726, 4.64474196, 1.97727022, 4.86978148, 2.62207358, 1, 4.06654206)

```

Time Horizon is the number of allocations we have available synonymous to number of experimental units in an experiment. 
In this simulation, we set time horizon to 2000 units.

We run this simulation for 20 different times using various seeds.



```{r echo=FALSE, cache=FALSE, warning=FALSE}
setwd("G:\\My Drive\\Research\\Bandits\\code\\bandits\\ucb_inf\\Output")

xi_simulation <- TRUE


group_outcome <- read.csv("group_outcome_1.csv") %>% 
  group_by(ite,alg) %>% mutate(x=row_number()) %>% filter(x<2001) %>% 
  ungroup()

regret_mse <- read.csv("regret_1.csv") %>% 
  group_by(ite,alg) %>% mutate(x=row_number()) %>% filter(x<2001) %>% ungroup()

prop_mse <- read.csv("ipw_aipw_prop.csv") %>% 
  group_by(ite,alg) %>% mutate(x=row_number()) %>% ungroup()


if (xi_simulation) {
  xi_group <- read.csv("group_eps_sim.csv")
  xi_regret <- read.csv("regret_mse_eps_sim.csv")
}



# Setting new WD so that results are saved in different folder
setwd("G:\\My Drive\\Research\\Bandits\\code\\bandits\\ucb_inf\\results")


```

# Group allocation graphs


We first look at the allocations for widely used algorithms.

* ab - AB Testing
* eps - Epsilon Greedy
* thomp - Thompson Sampling
* ucb - Upper confidence bound


```{r echo=FALSE, warning=FALSE}

df <- group_outcome %>% filter(ite==0) 

df_regret <- regret_mse %>% filter(ite==0)

main_algs <- c("ab", "ucb", "thomp", "eps")

ggplot(df %>% filter(alg == main_algs)) + geom_point(aes(x=x, y=factor(group)), shape=1, alpha=0.6) +
  facet_grid(alg ~.) +
  theme_bw() +
  labs(x='Time Period', y = 'Group') +
  theme(axis.text.y = element_text(size = rel(0.7))) 
# +   ggsave("group_all.png", width = 10, height = 6, dpi=300, units="in")

```

UCB is the most efficient. Looks like Thompson sampling is not as efficient as perceived. In this case, it is hovering between 8, 4, 0

### Average number of optimal/sub-optimal allocations in each algorithm


```{r echo=FALSE, warning=FALSE}

df_alloc <- group_outcome %>% filter(alg == main_algs) %>% 
  mutate(group = ifelse(group==8, 'Optimal', 'SubOptimal') )%>% group_by(alg, group, ite) %>% count() %>% group_by(alg, group) %>% summarise(mean_alloc = mean(n))

ggplot(df_alloc, aes(x=alg, y=mean_alloc, fill=group)) + geom_bar(stat="identity", position="dodge") +
  theme_bw()  +
  labs(title = "Average number of Optimal/SubOptimal allocations")

```

UCB is the most optimal as it is efficient. AB does random allocation, so, it is the least optimal.


## Group allocations for Inference based algorithms algorithms.

For inference based allocation, we deviate from the main allocation algorithm with the probability of $\epsilon_n$ defined by


$\eta = \frac{\sum{\frac{\sigma^2}{n}}}{\xi K}$
$\epsilon_n = \frac{\eta}{1+\eta}$

$\xi$ and $K$ are user defined parameters that we will discuss later.

The main idea is that by deviating from main algorithm and making inference based allocations, we learn more about an arm. Learning here implies reducing standard error of an arm. This ofcourse comes at the cost of Utility.


```{r echo=FALSE, warning=FALSE}


adv_algs <- c("ucb", "ucb_inf_eps", "thomp", "thomp_inf_eps")

ggplot(df %>% filter(alg == adv_algs)) + geom_point(aes(x=x, y=factor(group)), shape=1, alpha=0.6) +
  facet_grid(alg ~.) +
  theme_bw() +
  labs(x='Time Period', y = 'Group') +
  theme(axis.text.y = element_text(size = rel(0.7))) 

```


"_inf_eps" suffix suggests that the algorithms makes inference based allocation with some probability. In these algorithms, the allocations are not as concentrated around few arms as those of the main algorithms (UCB, Thompson Sampling).

#### Average number of optimal/sub-optimal allocations in each algorithm


```{r echo=FALSE, warning=FALSE}

df_alloc <- group_outcome %>% filter(alg == adv_algs) %>% 
  mutate(group = ifelse(group==8, 'Optimal', 'SubOptimal') )%>% group_by(alg, group, ite) %>% count() %>% group_by(alg, group) %>% summarise(mean_alloc = mean(n))

ggplot(df_alloc, aes(x=alg, y=mean_alloc, fill=group)) + geom_bar(stat="identity", position="dodge") +
  theme_bw()

```

UCB_INF_EPS is slightly worse compared to UCB but is not as bad as Thompson Sampling based algorithms


The relative performance of these algorithms can be formalized in regret analysis.


# Regret Analysis


```{r echo=FALSE, warning=FALSE}
ggplot(df_regret, aes(x=x, y=regret)) + geom_line(aes(color=alg)) +
  labs(title = "Regret") + xlim(0,2500) +
  geom_dl(aes(label=alg), method=list('last.points', cex=0.8)) +
  theme_bw() 
 # geom_text_repel(aes(label=alg)) +
 # ggsave("regret_eps_n.png", width = 10, height = 6, dpi=300, units="in")


```

UCB algorithm is the most efficient. One interesting observation is that ucb_inf_eps performs better than Thompson Sampling. This could be because of a bad seed (one random bad simulation). So, we check to see if this the same case for other iterations (other random seeds).


```{r echo=FALSE, warning=FALSE}

thomp_regret <- regret_mse %>% filter(alg=='thomp') %>% mutate(ite=factor(ite))


ggplot(thomp_regret, aes(x=x, y=regret)) + geom_line(aes(color=ite)) +
  labs(title = "Regret for Different Thompson Sampling Iterations") + xlim(0,2200) +
  geom_dl(aes(label=ite, size=1), method=list('last.points', cex=0.5)) +
  theme_bw()
 # geom_text_repel(aes(label=alg)) +
 # ggsave("regret_eps_n.png", width = 10, height = 6, dpi=300, units="in")

```


There is a huge variance among iterations, however, 0th iteration which was considered in comparison with other algorithms, sits right in the middle, this means we did not get an extreme performing iteration. What we have is reasonable.

The average performing iteration (ite=6) has regret of 1000, which seems to be very similar to regret of Epsilon_greedy


# Mean Squared error of mean estimates

At each time horizong we use sample mean to estimate mean of each arm and calculate error = est.mean-true.mean.

$MSE = \sum_{k} (error_k)^2$


```{r echo=FALSE, warning=FALSE}
df_prop <- prop_mse %>% filter(ite==0) %>% rename(mean_mse = mse) %>% select(-c(mean_est, group))
df_mse <- df_regret %>% select(-c(regret, var_mse)) 
mse <- rbind(df_prop, df_mse)


ggplot(mse %>% filter(alg==main_algs),aes(x=x, y=mean_mse)) + geom_line(aes(color=alg)) +
  labs(title = "MSE of Mean") +  xlim(0,2200) +
  geom_dl(aes(label=alg), method=list('last.points', cex=0.8)) +
  theme_bw()

```


UCB and Thompson Sampling have higher MSE compared to AB or Epsilon Greedy. This means that the estimates of AB and Epsilon-Greedy are more accurate.


```{r echo=FALSE, warning=FALSE}

less_mse_algs = c("ab", "eps", "ucb_inf_eps", "thomp_inf_eps")
this_algs <- c('ucb', 'thomp', less_mse_algs)

ggplot(mse %>% filter(alg==this_algs),aes(x=x, y=mean_mse)) + geom_line(aes(color=alg)) +
  labs(title = "MSE of Mean") +  xlim(0,2200) +
  geom_dl(aes(label=alg), method=list('last.points', cex=0.8)) +
  theme_bw()

```
The Inference based algorithms are performing much better than Bandit algorithms in terms if MSE and are comparable to traditional algorithms.

Difference between the above algorithms is clear during later allocations. So we limit the Y axis to see these differences.

```{r echo=FALSE, warning=FALSE}

ggplot(mse%>% filter(alg==less_mse_algs), aes(x=x, y=mean_mse)) + geom_line(aes(color=alg)) + labs(title = "MSE of Mean Zoomed") +   geom_dl(aes(label=alg), method=list('last.points', cex=0.7)) + xlim(0, 2200) + ylim(0,2)+
  theme_bw()

ggplot(mse%>% filter(alg==less_mse_algs), aes(x=x, y=mean_mse)) + geom_line(aes(color=alg)) + labs(title = "MSE of Mean Zoomed") +   geom_dl(aes(label=alg), method=list('last.points', cex=0.7)) + xlim(0, 2200) + ylim(0,0.5)+
  theme_bw()

```

AB, UCB_INF_EPS, THOMP_INF_EPS perform the best and almost indistingushable. However, INF_EPS algorithms perform as good as AB.

## MSE of Weighed estimators.

We also want to see how the weighed estimators perform wrt original MAB algorithms and their INF versions. We use two weighed estimators.

* ipw - Inverse propensity weighing
* aipw - Augmented inverse propensity weighing

These are not new MAB algorithms. The are only estimators to calculate mean of already existing algorithm. The main reason to use these estimators is for their unbiasedness properties. Theoretically, IPW is unbiased but had high variance and AIPW is unbiased and has lower variance. 

In order to use IPW and AIPW, we need to know the propensity of choosing an arm at every allocation. Propensity of every arm at every time period can be calculated for Thompson Sampling using Monte Carlo simulations but it cannot be calculated for UCB. So we only use weighed estimators for TS.


```{r echo=FALSE, warning=FALSE}

weight_algs = c("thomp_ipw", "thomp_aipw", "thomp", "thomp_inf_eps_ipw", "thomp_inf_eps_aipw", "thomp_inf_eps")

ggplot(mse %>% filter(alg==weight_algs),aes(x=x, y=mean_mse)) + geom_line(aes(color=alg)) +
  labs(title = "MSE of Mean")+
  theme_bw()


ggplot(mse%>% filter(alg==weight_algs), aes(x=x, y=mean_mse)) + geom_line(aes(color=alg)) + labs(title = "MSE of Mean Zoomed") + geom_dl(aes(label=alg), method=list('last.points', cex=0.5)) + xlim(0, 2500) + ylim(0,10)+
  theme_bw()

ggplot(mse%>% filter(alg==weight_algs), aes(x=x, y=mean_mse)) + geom_line(aes(color=alg)) + labs(title = "MSE of Mean Zoomed") + geom_dl(aes(label=alg), method=list('last.points', cex=0.5)) + xlim(0, 2500) + ylim(0,3)+
  theme_bw()
```

MSE of weighed estimators is very high at the start owing to lower propensity of arm selctions increasing the variance of estimate. This slowly goes down as we gather more data (horizon increase).

Also, the MSE of weighed estimators seems to be *higher* than the non-weighed counterparts. This is interesting.

The main property of these algorithms is that their estimators are unbiased (in expectation). But, for one single iteration, the MSE seems to be worse than un-weighed counterpart.


### Compare MSE of Weighed estimators with UCB_INF_EPS.

We also want to see how the weighed estimators perform wrt original MAB algorithms and their INF versions.


```{r echo=FALSE, warning=FALSE}

compare_weight_algs = c("ucb_inf_eps", weight_algs)

ggplot(mse %>% filter(alg==compare_weight_algs),aes(x=x, y=mean_mse)) + geom_line(aes(color=alg)) +
  labs(title = "MSE of Mean")+
  theme_bw()


ggplot(mse%>% filter(alg==compare_weight_algs), aes(x=x, y=mean_mse)) + geom_line(aes(color=alg)) + labs(title = "MSE of Mean Zoomed") + geom_dl(aes(label=alg), method=list('last.points', cex=0.5)) + xlim(0, 2500) + ylim(0,10)+
  theme_bw()

ggplot(mse%>% filter(alg==compare_weight_algs), aes(x=x, y=mean_mse)) + geom_line(aes(color=alg)) + labs(title = "MSE of Mean Zoomed") + geom_dl(aes(label=alg), method=list('last.points', cex=0.5)) + xlim(0, 2500) + ylim(0,3)+
  theme_bw()


```

UCB_INF_EPS, THOMP_INF_EPS performs better than even Weighed algorithms.


# $\xi$ simulation for ucb_inf_eps

$\eta = \frac{\sum{\frac{\sigma^2}{n}}}{\xi K}$
$\epsilon_n = \frac{\eta}{1+\eta}$



```{r echo=FALSE, eval=xi_simulation, warning=FALSE}


xi_group_sub <- xi_group %>% mutate(chi = round(chi,2)) %>%
  group_by(chi) %>% mutate(x=row_number()) %>% ungroup()


ggplot(xi_group_sub) +
  geom_point(aes(x=x, y=factor(group)), shape=1, alpha=0.6) +
  facet_grid(chi ~.) + theme_bw() +
  labs(x='Time Period', y = 'Group') +
  theme(axis.text.y = element_text(size = 6)) + 
  theme(axis.text.y = element_text(size = rel(1))) 

xi_regret_sub <- xi_regret %>% 
  mutate(chi = factor(round(chi,2))) %>% group_by(chi) %>% 
  mutate(x=row_number())

ggplot(xi_regret_sub, aes(x=x, y=regret)) + 
  geom_line( aes(color=chi)) + theme_bw() + geom_dl(aes(label=chi), method=list('last.points', cex=0.5)) +labs(title = 'Regret vs Xi')

ggplot(xi_regret_sub, aes(x=x, y=mean_mse)) + 
  geom_line(aes(color=chi)) + geom_dl(aes(label=chi), method=list('last.points', cex=0.5)) +
  theme_bw() + ylim(0,0.25) +labs(title = 'MSE Zoomed')

```

$\xi$ serves as a knob for practitioner to control the amount of exploration in INF_EPS. $\xi$ directly effects $\epsilon$ which in turn effects the amount of exploration. Higher the value of $\xi$, lower the amount of exploration.


# Small sample properties of algortihms


## Best arm bias

```{r echo=FALSE, warning=FALSE}

# calculate bias

# Best arm of only AB and UCB

means <- group_outcome  %>% group_by(group, alg, ite) %>% summarise(mn = mean(outcome)) %>% ungroup()

weighed_means <- prop_mse %>% group_by(group, alg, ite) %>% summarise(mn = mean(mean_est)) %>% ungroup()


all_means <- rbind(means, weighed_means)

best <- all_means %>% filter(group==which.max(true_means)-1) %>% 
  mutate(bias = mn-max(true_means)) %>% group_by(alg) %>%
  summarise(Bias = mean(bias), sd = sd(bias)/sqrt(n()))


ggplot(best, aes(x=reorder(alg, abs(Bias)), y=Bias)) + geom_bar(stat="identity") +
  geom_errorbar(aes(ymin=Bias-qnorm(0.975)*sd, ymax=Bias+qnorm(0.975)*sd), width=0.2) +
  theme_bw()  + ylim(-0.25, 0.25) + theme(axis.title.x=element_blank()) +
  geom_hline(yintercept = 0) + labs(title = "Bias of best arm") + theme(axis.text.x = element_text(angle = 90)) 
 # ggsave("best_bias_2.png", width = 5, height = 5, scale = 0.7, units="in")
```

Weighed estimators have the highest bias, even for the best arm. 
This might be because of their high variance properties. As we only have 20 iterations, the bias might not be still converging to 0.


```{r echo=FALSE, warning=FALSE}

ggplot(best, aes(x=reorder(alg, abs(Bias)), y=Bias)) + geom_bar(stat="identity") +
  theme_bw()  + ylim(-0.025, 0.025) + theme(axis.title.x=element_blank()) +
  geom_hline(yintercept = 0) + labs(title = "Bias of best arm") + theme(axis.text.x = element_text(angle = 90)) 
```

Although, looking at the relative values, might be deceiving. As the highest value of bias in 0.017, which is 0.3% of the actual value.


## Worst arm bias

```{r echo=FALSE, warning=FALSE}


# Worst arm of only AB and UCB


worst <- all_means %>% filter(group==which.min(true_means)-1) %>% 
  mutate(bias = mn-min(true_means)) %>% group_by(alg) %>%
  summarise(Bias = mean(bias), sd = sd(bias)/sqrt(n()))


ggplot(worst, aes(x=reorder(alg, abs(Bias)), y=Bias)) + geom_bar(stat="identity") +
  geom_errorbar(aes(ymin=Bias-qnorm(0.975)*sd, ymax=Bias+qnorm(0.975)*sd), width=0.2) +
  theme_bw()  + ylim(-1.6, 1.6) + theme(axis.title.x=element_blank()) +
  geom_hline(yintercept = 0) + labs(title = "Bias of Worst arm") + theme(axis.text.x = element_text(angle = 90)) 

```

The relative value of bias is high for the worst arm.

*One thing to note is that, for thompson sampling based algorithms, we have to choose priors and update posteriors based on rewards, This is problem for worst arm. If we choose bad priors and they barely get allocations, then Bias and MSE will be high, even after using weighed estimators*

This makes thompson sampling even with weighed estimators, undesirable for practical use of inference.

However, for INF_EPS based algorithms, this seem to correct with time as more allocations come from Inference allocations.



## MSE of Best arm 

```{r echo=FALSE, warning=FALSE}

best <- all_means %>%  filter(group==which.max(true_means)-1)  %>% group_by(alg) %>%
  summarise(m=mean(mn), v=var(mn)) %>% mutate(mse=(m-max(true_means))^2+v)


ggplot(best, aes(x=reorder(alg, mse), y=mse)) + geom_bar(stat="identity") +
  theme_bw()  + ylim(-0.2, 0.2) + theme(axis.title.x=element_blank()) +
  geom_hline(yintercept = 0) + labs(title = "MSE of best arm") + theme(axis.title.x=element_blank()) + theme(axis.text.x = element_text(angle = 90)) 


```

MSE is very very low for all algoritms in terms of scale, the only high values seem to be for weighed estimators.

## MSE of Worst arm 


```{r echo=FALSE, warning=FALSE}


worst <- all_means %>%  filter(group==which.min(true_means)-1)  %>% group_by(alg) %>%
  summarise(m=mean(mn), v=var(mn)) %>% mutate(mse=(m-min(true_means))^2)


ggplot(worst, aes(x=reorder(alg, mse), y=mse)) + geom_bar(stat="identity") +
  theme_bw()  + ylim(-1, 1) + theme(axis.title.x=element_blank()) +
  geom_hline(yintercept = 0) + labs(title = "MSE of Worst arm") + theme(axis.title.x=element_blank()) + theme(axis.text.x = element_text(angle = 90)) 

```

Again, for vanilla MAB algorithms and their weighes estimators, MSE is high. This seem to reduce for INF_EPS algorithms.

```{r echo=FALSE, warning=FALSE}
dat = worst %>% filter(alg  %in% c('ab', 'eps', 'ucb_inf_eps', 'thomp_inf_eps_ipw'))

ggplot(dat, aes(x=reorder(alg, mse), y=mse)) + geom_bar(stat="identity") +
  theme_bw()  + ylim(-0.05, 0.05) + theme(axis.title.x=element_blank()) +
  geom_hline(yintercept = 0) + labs(title = "MSE of Worst arm") + theme(axis.title.x=element_blank()) + theme(axis.text.x = element_text(angle = 90)) 

```


# Conclusions

+ UCB_INF_EPS is clear winner for use cases where there is relative importance for both efficiency and Inference. 
+ Weighed Estimators of existing MAB algorithms may have nice properties *in expectation* but for practical purposes when the practitioner has one shot, UCB_INF_EPS will outperform in regret minimization and estimation.
+ This problem is exacerbated if bad priors are chosen for Thompson Sampling based algorithms. 
